# -*- coding: utf-8 -*-
"""Fake account Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wilK1C63EKw-8eTeDYMnggckh3FjlbJ8
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import pandas as pd

fake_accounts = pd.read_csv(
    "fake_account.csv",
    sep="\t",
    header=None,
    names=["user_id", "text"]
)

legitimate_accounts = pd.read_csv(
    "legitimate_account.csv",
    sep="\t",
    header=None,
    names=["user_id", "text"]
)

# Add labels
fake_accounts["label"] = 1        # fake
legitimate_accounts["label"] = 0  # legitimate

# Combine
dataset = pd.concat([fake_accounts, legitimate_accounts], ignore_index=True)

print(dataset.head())
print(dataset.info())

fake_accounts = pd.read_csv(
    "fake_account.csv",
    sep="\t",
    header=None,
    names=["user_id", "text"]
)

fake_accounts["reposts"] = 0
fake_accounts["comments"] = 0
fake_accounts["likes"] = 0
fake_accounts["label"] = 1   # fake

legitimate_accounts = pd.read_csv(
    "legitimate_account.csv",
    sep="\t",
    header=None,
    names=["user_id", "timestamp", "reposts", "comments", "likes", "text"]
)

legitimate_accounts["label"] = 0   # legitimate

dataset = pd.concat([fake_accounts, legitimate_accounts], ignore_index=True)

print(dataset.head())
print(dataset.info())

# Fill missing text
dataset["text"] = dataset["text"].fillna("")

# Convert timestamp
dataset["timestamp"] = pd.to_datetime(dataset.get("timestamp"), errors="coerce")

# Feature engineering
dataset["text_length"] = dataset["text"].apply(len)
dataset["has_url"] = dataset["text"].str.contains("http", case=False).astype(int)

# Fill numeric NaNs
for col in ["reposts", "comments", "likes"]:
    dataset[col] = dataset[col].fillna(0)

from sklearn.model_selection import train_test_split

# Separate features and target
X = dataset.drop(columns=['label'])
y = dataset['label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import StandardScaler

# Select only numeric columns
numeric_cols = ['reposts', 'comments', 'likes', 'text_length', 'has_url']

X_train_num = X_train[numeric_cols]
X_test_num = X_test[numeric_cols]

# Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_num)
X_test_scaled = scaler.transform(X_test_num)

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=100, random_state=42)

clf.fit(X_train_scaled, y_train)

y_pred = clf.predict(X_test_scaled)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

importances = clf.feature_importances_

feature_names = numeric_cols  # same list used before

feature_importances = (
    pd.Series(importances, index=feature_names)
    .sort_values(ascending=False)
)

print("Feature Importances:\n", feature_importances)

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

# TF-IDF on text
tfidf = TfidfVectorizer(
    max_features=5000,
    stop_words='english'
)

X_train_text = tfidf.fit_transform(X_train['text'])
X_test_text = tfidf.transform(X_test['text'])

# Numeric features
X_train_num = X_train[numeric_cols].values
X_test_num = X_test[numeric_cols].values

# Combine text + numeric
X_train_final = hstack([X_train_text, X_train_num])
X_test_final = hstack([X_test_text, X_test_num])

from sklearn.ensemble import RandomForestClassifier

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from scipy.sparse import hstack

numeric_cols = ['reposts', 'comments', 'likes', 'text_length', 'has_url']

from sklearn.model_selection import train_test_split

X = dataset.drop(columns=['label'])
y = dataset['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from scipy.sparse import hstack

numeric_cols = ['reposts', 'comments', 'likes', 'text_length', 'has_url']

tfidf = TfidfVectorizer(
    max_features=5000,
    stop_words='english'
)

X_train_text = tfidf.fit_transform(X_train['text'])
X_test_text = tfidf.transform(X_test['text'])

X_train_num = X_train[numeric_cols].values
X_test_num = X_test[numeric_cols].values

X_train_final = hstack([X_train_text, X_train_num])
X_test_final = hstack([X_test_text, X_test_num])

clf = LogisticRegression(max_iter=1000, n_jobs=-1)
clf.fit(X_train_final, y_train)

y_pred = clf.predict(X_test_final)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

def report_fake_account(user_id, platform):
    report = {
        "user_id": user_id,
        "platform": platform,
        "status": "Reported",
        "action": "Pending review by Central Agency"
    }
    return report

report_fake_account("793c7dcfa5cbde650e80b07cb77b73ad", "Twitter")

central_agency_db = []

def send_to_central_agency(report):
    central_agency_db.append(report)
    return "Received by Central Cyber Security Agency"

import hashlib
import json

def generate_block(report):
    block_string = json.dumps(report, sort_keys=True)
    block_hash = hashlib.sha256(block_string.encode()).hexdigest()
    return block_hash

def platform_action(user_id, confidence):
    if confidence > 0.8:
        return "Account Suspended"
    elif confidence > 0.6:
        return "Account Under Review"
    else:
        return "No Action"

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 1. Setup a Mock Dataset (In a real scenario, you'd load a CSV here)
# Features: [Followers, Following, PostCount, IsVerified(0/1)]
data = {
    'followers': [100, 5000, 10, 20, 10000, 5],
    'following': [200, 400, 2000, 1500, 500, 3000],
    'posts': [50, 100, 1, 0, 200, 2],
    'is_fake': [0, 0, 1, 1, 0, 1]  # 0 = Real, 1 = Fake
}
df = pd.DataFrame(data)

# 2. Define Features (X) and Target (y)
X = df.drop('is_fake', axis=1)
y = df['is_fake']

# 3. Create and Train the Model
model = RandomForestClassifier(n_estimators=100)
model.fit(X, y)

print("Model has been defined and trained successfully!")

# 4. Define 'test_data' for the next step
# Let's test an account with 5 followers but following 2000 people (Suspicious!)
test_data = [[5, 2000, 1]]

import hashlib
import time

# --- FUNCTION DEFINITION ---
def generate_blockchain_record(account_id, platform):
    timestamp = time.ctime()
    evidence_string = f"{account_id}-{platform}-{timestamp}"
    evidence_hash = hashlib.sha256(evidence_string.encode()).hexdigest()

    print(f"--- Blockchain Entry Created ---")
    print(f"Account ID: {account_id}")
    print(f"Evidence Hash: {evidence_hash}")
    return evidence_hash

# --- DETECTION LOGIC (This defines 'status') ---
# In a real scenario, this 'prediction' comes from your ML model
# For now, let's simulate a detection:
prediction = [1] # 1 means Fake, 0 means Real

if prediction[0] == 1:
    status = "FAKE"
    print("System Alert: Fake account detected!")
else:
    status = "REAL"
    print("System Check: Account appears legitimate.")

# --- BLOCKCHAIN TRIGGER ---
# Now 'status' exists, so this won't throw an error
if status == "FAKE":
    evidence_id = generate_blockchain_record("User_XYZ_99", "Instagram")
else:
    evidence_id = None

def notify_central_agency(acc_name, platform, proof_hash):
    # This simulates sending data to the ITBP/CERT-In portal
    report = {
        "Agency": "Central Social Media Monitoring Cell (CSMMC)",
        "Priority": "High (Security Threat)",
        "Account": acc_name,
        "Platform": platform,
        "Blockchain_Proof": proof_hash,
        "Action": "Request for Suspension under IT Act Section 69A"
    }

    print("### OFFICIAL REPORT GENERATED ###")
    for key, value in report.items():
        print(f"{key}: {value}")
    print("---------------------------------")
    print("Status: Sent to Social Media Organization for Deletion.")

if status == "FAKE":
    notify_central_agency("User_XYZ_99", "Instagram", evidence_id)

import pandas as pd
import hashlib
import time

# --- STEP 1: LOAD YOUR ACTUAL DATA ---
# This matches the 'fake_account.csv' structure you uploaded
try:
    df = pd.read_csv('fake_account.csv', sep='\t', header=None, names=['User_ID', 'Post_Content'])
    print(f"Successfully loaded {len(df)} posts from {df['User_ID'].nunique()} unique accounts.")
except FileNotFoundError:
    print("Error: 'fake_account.csv' not found. Please ensure it is uploaded to the root folder.")

# --- STEP 2: BLOCKCHAIN LOGGING ENGINE ---
def generate_blockchain_evidence(user_id, platform="Social Media"):
    """Creates an immutable hash of the account and detection timestamp."""
    timestamp = time.ctime()
    # Create a unique digital fingerprint (evidence)
    evidence_string = f"Suspect:{user_id}|Platform:{platform}|Time:{timestamp}"
    evidence_hash = hashlib.sha256(evidence_string.encode()).hexdigest()

    print(f"\n[BLOCKCHAIN LAYER]")
    print(f">> Evidence Hash Generated: {evidence_hash}")
    return evidence_hash

# --- STEP 3: CENTRAL AGENCY REPORTING ---
def notify_central_agency(user_id, evidence_hash):
    """Generates the official report for ITBP/CERT-In for worldwide suspension."""
    print(f"\n[CENTRAL AGENCY ACTION]")
    print(f"RE: Official Request for Account Suspension")
    print(f"Target Account ID: {user_id}")
    print(f"Blockchain Verification ID: {evidence_hash}")
    print(f"Status: Request sent to Social Site Organization for Time-Bound Deletion.")

# --- STEP 4: RUN DETECTION & WORKFLOW ---
# We pick a specific User ID from your CSV to analyze
# (Example: 'c3230836350' from your file)
target_user = "c3230836350"

# Detection Logic: In a real scenario, an ML model would analyze the 'Post_Content'.
# For this tool, we simulate the detection of a high-risk account.
is_fake = True  # Simulation: Let's assume this user was flagged by NLP analysis

if is_fake:
    print(f"ALERT: User {target_user} identified as a FAKE/SPAM account.")

    # 1. Secure evidence using Blockchain
    proof_hash = generate_blockchain_evidence(target_user)

    # 2. Trigger the Central Agency workflow
    notify_central_agency(target_user, proof_hash)
else:
    print(f"Status: User {target_user} appears legitimate.")

import pandas as pd
import hashlib
import time

# --- 1. DATA ORGANIZATION ---
# Legitimate data usually has more metadata (timestamps, counts)
# Fake data (like your CSV) often just has IDs and repetitive text
print("System: Loading Legitimate and Suspicious datasets...")

# --- 2. THE DETECTION TOOL (The Logic) ---
def detect_account_type(user_id, post_content):
    """
    In your full project, this would be an ML model.
    Logic: If the content is repetitive or lacks metadata, it's flagged.
    """
    # Example detection: If the ID is from your 'fake_account.csv' sample
    fake_list = ['c3230836350', 'c2741362562']

    if user_id in fake_list:
        return "FAKE"
    return "LEGITIMATE"

# --- 3. BLOCKCHAIN & REPORTING SYSTEM ---
class CyberSecurityTool:
    def __init__(self):
        self.blockchain_ledger = []

    def secure_evidence(self, user_id, reason):
        ts = time.time()
        # Create a secure hash (SHA-256) - This is the "Blockchain" part
        evidence_raw = f"{user_id}|{reason}|{ts}"
        evidence_hash = hashlib.sha256(evidence_raw.encode()).hexdigest()
        self.blockchain_ledger.append(evidence_hash)
        return evidence_hash

    def report_to_indian_agency(self, user_id, p_hash):
        print(f"\n[!] ALERT: CENTRAL AGENCY NOTIFIED")
        print(f"[*] Target ID: {user_id}")
        print(f"[*] Proof of Identification (Hash): {p_hash}")
        print(f"[*] Action: Time-bound suspension request sent to Platform HQ.")

# --- 4. EXECUTION ---
tool = CyberSecurityTool()

# Test Case 1: A user from your fake_account.csv
user_to_check = "c3230836350"
verdict = detect_account_type(user_to_check, "Repetitive content...")

if verdict == "FAKE":
    print(f"\nAccount {user_to_check} flagged as FAKE by Detection Tool.")
    # Secure it on the blockchain
    proof = tool.secure_evidence(user_to_check, "Pattern Mismatch")
    # Report to the Central Agency (ITBP Requirement)
    tool.report_to_indian_agency(user_to_check, proof)
else:
    print(f"\nAccount {user_to_check} verified as LEGITIMATE.")

import pandas as pd
import hashlib
import time

# 1. THE DATA LOADER (Handles both formats)
def process_social_data(file_path, is_legit=False):
    if is_legit:
        # Legitimate data has 6 columns (ID, Date, etc.)
        cols = ['User_ID', 'Timestamp', 'Likes', 'Shares', 'Comments', 'Content']
        return pd.read_csv(file_path, sep='\t', header=None, names=cols, encoding='utf-8')
    else:
        # Fake data has 2 columns (ID, Content)
        cols = ['User_ID', 'Content']
        return pd.read_csv(file_path, sep='\t', header=None, names=cols, encoding='utf-8')

# 2. THE CORE TOOL ENGINE (The "Identification" logic)
def run_itbp_detection_tool(user_id, content, metadata_count):
    # Rule-based detection for the problem statement
    # Fake accounts usually lack metadata (Likes/Shares/Comments)
    if metadata_count == 0:
        return "SUSPICIOUS (Possible Bot)"
    return "VERIFIED"

# 3. BLOCKCHAIN & REPORTING (The "Action" logic)
def blockchain_report_to_agency(user_id, status):
    if "SUSPICIOUS" in status:
        # Create Blockchain Hash
        evidence_hash = hashlib.sha256(f"{user_id}{time.time()}".encode()).hexdigest()

        print(f"\n--- [ITBP CYBER TOOL REPORT] ---")
        print(f"Target ID: {user_id}")
        print(f"Verdict: {status}")
        print(f"Blockchain Evidence ID: {evidence_hash}")
        print(f"Action: Forwarding to Central Agency for Worldwide Deletion.")
        return evidence_hash
    return None

# --- EXECUTION EXAMPLE ---
# Let's simulate checking one user from your 'fake_account.csv'
sample_user = "c3230836350"
sample_content = "Repetitive spam text..."
metadata_available = 0 # Your fake_account.csv doesn't have the 4 numeric columns

# Run the tool
result = run_itbp_detection_tool(sample_user, sample_content, metadata_available)
blockchain_report_to_agency(sample_user, result)

import pandas as pd
# Load your big file
df = pd.read_csv('fake_account.csv', sep='\t', header=None)
# Take only the first 10,000 rows (this will be roughly 5-10MB)
df_small = df.head(10000)
# Save it
df_small.to_csv('fake_account_small.csv', sep='\t', index=False, header=None)